{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12667459,"sourceType":"datasetVersion","datasetId":8005020}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Check hardware info","metadata":{"id":"qMcBI3IEG1sn"}},{"cell_type":"code","source":"!rm serialization_merge.json serialization_vocab.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:03:11.795539Z","iopub.execute_input":"2025-08-27T12:03:11.796418Z","iopub.status.idle":"2025-08-27T12:03:11.923646Z","shell.execute_reply.started":"2025-08-27T12:03:11.796381Z","shell.execute_reply":"2025-08-27T12:03:11.922596Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"CPU","metadata":{"id":"Yez65mPTHMcY"}},{"cell_type":"code","source":"!lscpu","metadata":{"id":"XpUqAwZJGyRs","outputId":"3afaade7-24fb-4dad-f8b3-ca0323e02e5b","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:22:30.460637Z","iopub.execute_input":"2025-08-27T12:22:30.461069Z","iopub.status.idle":"2025-08-27T12:22:30.591622Z","shell.execute_reply.started":"2025-08-27T12:22:30.461023Z","shell.execute_reply":"2025-08-27T12:22:30.590698Z"}},"outputs":[{"name":"stdout","text":"Architecture:             x86_64\n  CPU op-mode(s):         32-bit, 64-bit\n  Address sizes:          46 bits physical, 48 bits virtual\n  Byte Order:             Little Endian\nCPU(s):                   4\n  On-line CPU(s) list:    0-3\nVendor ID:                GenuineIntel\n  Model name:             Intel(R) Xeon(R) CPU @ 2.20GHz\n    CPU family:           6\n    Model:                79\n    Thread(s) per core:   2\n    Core(s) per socket:   2\n    Socket(s):            1\n    Stepping:             0\n    BogoMIPS:             4399.99\n    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht sysc\n                          all nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xt\n                          opology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq\n                           ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt\n                           aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dno\n                          wprefetch pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust\n                           bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx s\n                          map xsaveopt arat md_clear arch_capabilities\nVirtualization features:  \n  Hypervisor vendor:      KVM\n  Virtualization type:    full\nCaches (sum of all):      \n  L1d:                    64 KiB (2 instances)\n  L1i:                    64 KiB (2 instances)\n  L2:                     512 KiB (2 instances)\n  L3:                     55 MiB (1 instance)\nNUMA:                     \n  NUMA node(s):           1\n  NUMA node0 CPU(s):      0-3\nVulnerabilities:          \n  Gather data sampling:   Not affected\n  Itlb multihit:          Not affected\n  L1tf:                   Mitigation; PTE Inversion\n  Mds:                    Mitigation; Clear CPU buffers; SMT Host state unknown\n  Meltdown:               Mitigation; PTI\n  Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode;\n                           SMT Host state unknown\n  Reg file data sampling: Not affected\n  Retbleed:               Mitigation; IBRS\n  Spec rstack overflow:   Not affected\n  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prct\n                          l\n  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointe\n                          r sanitization\n  Spectre v2:             Mitigation; IBRS; IBPB conditional; STIBP conditional;\n                           RSB filling; PBRSB-eIBRS Not affected; BHI SW loop, K\n                          VM SW loop\n  Srbds:                  Not affected\n  Tsx async abort:        Mitigation; Clear CPU buffers; SMT Host state unknown\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Memory","metadata":{"id":"pjhHdk3NHTqH"}},{"cell_type":"code","source":"!free -h","metadata":{"id":"VQl8bCOzG7Vj","outputId":"43452f64-17ed-475d-a1ff-03f790d530e5","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:22:34.354146Z","iopub.execute_input":"2025-08-27T12:22:34.354998Z","iopub.status.idle":"2025-08-27T12:22:34.481172Z","shell.execute_reply.started":"2025-08-27T12:22:34.354962Z","shell.execute_reply":"2025-08-27T12:22:34.480022Z"}},"outputs":[{"name":"stdout","text":"               total        used        free      shared  buff/cache   available\nMem:            31Gi       805Mi        21Gi       2.0Mi       9.3Gi        30Gi\nSwap:             0B          0B          0B\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"GPU","metadata":{"id":"8AuuiaecHWQS"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"m8qr65VlHCPZ","outputId":"152f2be2-af03-4223-e660-2977fb6f16d2","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:22:36.771784Z","iopub.execute_input":"2025-08-27T12:22:36.772542Z","iopub.status.idle":"2025-08-27T12:22:36.895918Z","shell.execute_reply.started":"2025-08-27T12:22:36.772507Z","shell.execute_reply":"2025-08-27T12:22:36.894785Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: line 1: nvidia-smi: command not found\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Download training data","metadata":{"id":"7168QJ9kGpDh"}},{"cell_type":"code","source":"!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt","metadata":{"id":"bW476Z2xFPG-","outputId":"a248d9c4-a615-4c35-a846-9d067ebb1e75","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T11:35:41.179643Z","iopub.execute_input":"2025-08-27T11:35:41.180401Z","iopub.status.idle":"2025-08-27T11:35:47.825780Z","shell.execute_reply.started":"2025-08-27T11:35:41.180363Z","shell.execute_reply":"2025-08-27T11:35:47.824894Z"}},"outputs":[{"name":"stdout","text":"--2025-08-27 11:35:41--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\nResolving huggingface.co (huggingface.co)... 18.244.202.68, 18.244.202.73, 18.244.202.60, ...\nConnecting to huggingface.co (huggingface.co)|18.244.202.68|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cas-bridge.xethub.hf.co/xet-bridge-us/645e8da96320b0efe40ade7a/02e40cc51c59a4bc6c51bd7bc9acda4316e208745be060558eaf500cd14e9f96?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250827%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250827T113541Z&X-Amz-Expires=3600&X-Amz-Signature=8268432855661474f64d3b8caf47066646031449023491f314a8f85b2fe45953&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27TinyStoriesV2-GPT4-train.txt%3B+filename%3D%22TinyStoriesV2-GPT4-train.txt%22%3B&response-content-type=text%2Fplain&x-id=GetObject&Expires=1756298141&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NjI5ODE0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDVlOGRhOTYzMjBiMGVmZTQwYWRlN2EvMDJlNDBjYzUxYzU5YTRiYzZjNTFiZDdiYzlhY2RhNDMxNmUyMDg3NDViZTA2MDU1OGVhZjUwMGNkMTRlOWY5NioifV19&Signature=HKOXBsOAesr55AiOcafEClcyZbqEzS22QeKKzHbprPIOgzL-8MraGtw%7EkRiAgnb-jEVtGTvlHUjGF%7Eue9Ybhly4yjNvXdudrftYww-ezk1psrX5CBhvSy48GV-YtygDKfbcvSnanIvJWEGMb4njIqYG4gTgZbE3uYyayVX63d1P6Z4NMxKAfIKP7tvA1EWQj6riasuTOmspor3jKkrUfOAe4mgx0jiPWemTbuSm1LVqP0XQeX4GyfIbRjQL1wJsldUvhrDomyKGm%7E7%7EeqTrAqqlbzeSCPA0LhSp34wyF3SEuiA19GwVO5KENIOB77tQhWk2ACByESQNDyyT9LrBIiw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n--2025-08-27 11:35:41--  https://cas-bridge.xethub.hf.co/xet-bridge-us/645e8da96320b0efe40ade7a/02e40cc51c59a4bc6c51bd7bc9acda4316e208745be060558eaf500cd14e9f96?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250827%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250827T113541Z&X-Amz-Expires=3600&X-Amz-Signature=8268432855661474f64d3b8caf47066646031449023491f314a8f85b2fe45953&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27TinyStoriesV2-GPT4-train.txt%3B+filename%3D%22TinyStoriesV2-GPT4-train.txt%22%3B&response-content-type=text%2Fplain&x-id=GetObject&Expires=1756298141&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NjI5ODE0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDVlOGRhOTYzMjBiMGVmZTQwYWRlN2EvMDJlNDBjYzUxYzU5YTRiYzZjNTFiZDdiYzlhY2RhNDMxNmUyMDg3NDViZTA2MDU1OGVhZjUwMGNkMTRlOWY5NioifV19&Signature=HKOXBsOAesr55AiOcafEClcyZbqEzS22QeKKzHbprPIOgzL-8MraGtw%7EkRiAgnb-jEVtGTvlHUjGF%7Eue9Ybhly4yjNvXdudrftYww-ezk1psrX5CBhvSy48GV-YtygDKfbcvSnanIvJWEGMb4njIqYG4gTgZbE3uYyayVX63d1P6Z4NMxKAfIKP7tvA1EWQj6riasuTOmspor3jKkrUfOAe4mgx0jiPWemTbuSm1LVqP0XQeX4GyfIbRjQL1wJsldUvhrDomyKGm%7E7%7EeqTrAqqlbzeSCPA0LhSp34wyF3SEuiA19GwVO5KENIOB77tQhWk2ACByESQNDyyT9LrBIiw__&Key-Pair-Id=K2L8F4GPSG1IFC\nResolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.33.4.123, 13.33.4.15, 13.33.4.109, ...\nConnecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.33.4.123|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2227753162 (2.1G) [text/plain]\nSaving to: ‘TinyStoriesV2-GPT4-train.txt’\n\nTinyStoriesV2-GPT4- 100%[===================>]   2.07G   336MB/s    in 6.3s    \n\n2025-08-27 11:35:47 (336 MB/s) - ‘TinyStoriesV2-GPT4-train.txt’ saved [2227753162/2227753162]\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"config file","metadata":{}},{"cell_type":"code","source":"%%file config_kaggle.yaml\nspecial_tokens:\n  - \"<|endoftext|>\"\nenable_log: False\nlog_path: \"gold.log\"\nserialization: True\nserialization_vocab_path: \"serialization_vocab.json\"\nserialization_merge_path: \"serialization_merge.json\"\ntraindata_path: \"/kaggle/working/TinyStoriesV2-GPT4-train.txt\"\nvocab_size: 10000\ngpt2_regex: True\nparallel: True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:22:57.273298Z","iopub.execute_input":"2025-08-27T12:22:57.273652Z","iopub.status.idle":"2025-08-27T12:22:57.281096Z","shell.execute_reply.started":"2025-08-27T12:22:57.273619Z","shell.execute_reply":"2025-08-27T12:22:57.279996Z"}},"outputs":[{"name":"stdout","text":"Overwriting config_kaggle.yaml\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"Tokenizer","metadata":{"id":"q0ScUKvPCv5g"}},{"cell_type":"code","source":"%%file tokenizer.py\nimport regex as re\nimport json\nimport logging\nfrom collections import defaultdict\nfrom typing import BinaryIO\nimport os\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport heapq\n\nPAT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\nPAT = re.compile(PAT_PATTERN)\n\n# Wrapper for heap comparasion, lexical greater\nclass _Desc:\n    # __slots__ = ['x']\n\n    def __init__(self, x):\n        self.x = x\n    \n    def __lt__(self, other):\n        \"\"\"\n        Overwrite reversice, lexical greater\n        \"\"\"\n        return self.x > other.x\n    \n\nclass Tokenizer:\n    def __init__(\n            self, \n            special_tokens: list[str] | None = None, \n            enable_log: bool = False, \n            log_path: str = \"\",\n            serialization: bool = False,\n            serialization_vocab_path: str | None = None,\n            serialization_merge_path: str | None = None,\n        ):\n        self.special_tokens = special_tokens or []\n        self.next_id = 0\n        self.vocab: dict[int, bytes] = {}\n        self.merge: list[tuple[bytes, bytes]] = []\n        self.enable_log = enable_log\n        self._heap = []\n\n        self.serialization = serialization\n        self.serialization_vocab_path = serialization_vocab_path\n        self.serialization_merge_path = serialization_merge_path\n\n        if enable_log:\n            if not log_path:\n                raise ValueError(\"Logging is enable but no log path was provided\")\n            \n            self._set_log_conifg(log_path)\n\n    @staticmethod\n    def _set_log_conifg(log_path: str):\n        logging.basicConfig(\n            filename=log_path, \n            filemode=\"w\", \n            level=logging.INFO, \n            format=\"%(message)s\"\n        )\n    \n    def dump_pair_count(self, pair_count: dict[tuple[bytes], int], merged_token: tuple[tuple[bytes], int], index: int):\n        if self.enable_log:\n            serial = { str(k): v for k, v in pair_count.items() }\n            serial_merged_token = {str(merged_token[0]): merged_token[1]}\n            logging.info(json.dumps({\"step\": index, \"pair\": serial, \"merged\": serial_merged_token}, ensure_ascii=False, sort_keys=True))\n    \n    def init_vocab(self):\n        self.vocab = {x: bytes([x]) for x in range (256)}\n        token_id_start = 256\n    \n        for i, special_token in enumerate(self.special_tokens):\n            s_bytes = special_token.encode(\"utf-8\")\n            special_token_id = token_id_start + i\n            self.vocab[special_token_id] = s_bytes\n        \n        self.next_id = special_token_id + 1\n    \n    def remove_special_tokens(self, text: str) -> list[str]:\n        stokens_escaped = [re.escape(stoken) for stoken in self.special_tokens]\n        return re.split(\"|\".join(stokens_escaped), text)\n    \n    def remove_special_tokens_static(text: str, special_tokens: list[str]) -> list[str]:\n        stokens_escaped = [re.escape(stoken) for stoken in special_tokens]\n        return re.split(\"|\".join(stokens_escaped), text)\n    \n    @staticmethod\n    def find_chunk_boundaries(\n        file: BinaryIO, \n        desired_num_chunks: int, \n        split_special_token: bytes\n    ) -> list[int]:\n        \"\"\"\n        Chunk the file into parts that can be counted independently.\n        May return fewer chunks if the boundaries end up overlapping.\n        \"\"\"\n        assert isinstance(split_special_token, bytes), (\n            \"Must represent special token as a bytestring\"\n        )\n    \n        # Get total file size in bytes\n        file.seek(0, os.SEEK_END)\n        file_size = file.tell()\n        file.seek(0)\n    \n        chunk_size = file_size // desired_num_chunks\n    \n        # Initial guesses for chunk boundary locations, uniformly spaced\n        # Chunks start on previous index, don't include last index\n        chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n        chunk_boundaries[-1] = file_size\n    \n        mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n    \n        for bi in range(1, len(chunk_boundaries) - 1):\n            initial_position = chunk_boundaries[bi]\n            file.seek(initial_position)  # Start at boundary guess\n            while True:\n                mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n    \n                # If EOF, this boundary should be at the end of the file\n                if mini_chunk == b\"\":\n                    chunk_boundaries[bi] = file_size\n                    break\n    \n                # Find the special token in the mini chunk\n                found_at = mini_chunk.find(split_special_token)\n                if found_at != -1:\n                    chunk_boundaries[bi] = initial_position + found_at\n                    break\n                initial_position += mini_chunk_size\n    \n        # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n        return sorted(set(chunk_boundaries))\n    \n    # @staticmethod\n    # def pretokenize_and_count(docs: list[str], gpt2_regex: bool = False) -> dict[tuple[bytes], int]:\n    #     token_count : dict[tuple[bytes], int] = {}\n    \n    #     for doc in docs:\n    #         pre_tokens = None\n    #         # Use a regex-based pre-tokenizer (used by GPT-2; Radford et al., 2019)\n    #         if gpt2_regex:\n    #             PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n    #             pre_tokens = re.finditer(PAT, doc)\n    #             pre_tokens = [match.group(0) for match in pre_tokens]\n    #         else:\n    #             pre_tokens = doc.split()\n    \n    #         for token in pre_tokens:\n    #             bytes_token = token.encode(\"utf-8\")\n                \n    #             tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n    #             token_count[tuple_bytes_token] = token_count.get(tuple_bytes_token, 0) + 1\n            \n    #     return token_count\n\n    # @staticmethod\n    # def pretokenize_and_count(docs: list[str], gpt2_regex: bool = False) -> dict[tuple[bytes], int]:\n    #     token_count : dict[tuple[bytes], int] = {}\n    #     # token_count_get = token_count.get\n    \n    #     for doc in docs:\n    #         pre_tokens = None\n    #         # Use a regex-based pre-tokenizer (used by GPT-2; Radford et al., 2019)\n    #         if gpt2_regex:\n    #             # PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n    #             pre_tokens = PAT.finditer(doc)\n    #             pre_tokens = [match.group(0) for match in pre_tokens]\n    #         else:\n    #             pre_tokens = doc.split()\n    \n    #         for token in pre_tokens:\n    #             bytes_token = token.encode(\"utf-8\")\n                \n    #             tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n    #             token_count[tuple_bytes_token] = token_count.get(tuple_bytes_token, 0) + 1\n            \n    #     return token_count\n\n    # @staticmethod\n    # def pretokenize_and_count(docs: list[str], gpt2_regex: bool = False) -> dict[tuple[bytes], int]:\n    #     token_count : dict[tuple[bytes], int] = {}\n    #     token_count_get = token_count.get\n    \n    #     for doc in docs:\n    #         pre_tokens = None\n    #         # Use a regex-based pre-tokenizer (used by GPT-2; Radford et al., 2019)\n    #         if gpt2_regex:\n    #             # PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n    #             pre_tokens = PAT.finditer(doc)\n    #             pre_tokens = [match.group(0) for match in pre_tokens]\n    #         else:\n    #             pre_tokens = doc.split()\n    \n    #         for token in pre_tokens:\n    #             bytes_token = token.encode(\"utf-8\")\n                \n    #             tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n    #             token_count[tuple_bytes_token] = token_count_get(tuple_bytes_token, 0) + 1\n            \n    #     return token_count\n    \n    # #OPED\n    # @staticmethod\n    # def pretokenize_and_count(docs: list[str], gpt2_regex: bool = False) -> dict[tuple[bytes], int]:\n    #     token_count : dict[tuple[bytes], int] = {}\n    #     token_count_get = token_count.get\n    \n    #     for doc in docs:\n    #         # pre_tokens = None\n    #         # Use a regex-based pre-tokenizer (used by GPT-2; Radford et al., 2019)\n    #         if gpt2_regex:\n    #             for token in PAT.finditer(doc):\n    #                 token_str = token.group(0)\n    #                 bytes_token = token_str.encode(\"utf-8\")\n\n    #                 tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n    #                 token_count[tuple_bytes_token] = token_count_get(tuple_bytes_token, 0) + 1\n    #         else:\n    #             for token in doc.split():\n    #                 bytes_token = token.encode(\"utf-8\")\n\n    #                 tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n    #                 token_count[tuple_bytes_token] = token_count_get(tuple_bytes_token, 0) + 1\n\n    #         # if gpt2_regex:\n    #         #     # PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n    #         #     pre_tokens = PAT.finditer(doc)\n    #         #     pre_tokens = [match.group(0) for match in pre_tokens]\n    #         # else:\n    #         #     pre_tokens = doc.split()\n    \n    #         # for token in pre_tokens:\n    #         #     bytes_token = token.encode(\"utf-8\")\n                \n    #         #     tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n    #         #     token_count[tuple_bytes_token] = token_count_get(tuple_bytes_token, 0) + 1\n            \n    #     return token_count\n    \n    # @staticmethod\n    # def pretokenize_and_count(docs: list[str], gpt2_regex: bool = False) -> dict[tuple[bytes], int]:\n    #     token_count : dict[tuple[bytes], int] = {}\n    #     token_count_get = token_count.get\n    \n    #     for doc in docs:\n    #         # pre_tokens = None\n    #         # Use a regex-based pre-tokenizer (used by GPT-2; Radford et al., 2019)\n    #         if gpt2_regex:\n    #             for token in PAT.finditer(doc):\n    #                 token_str = token.group(0)\n    #                 bytes_token = token_str.encode(\"utf-8\")\n\n    #                 length_bytes_token = len(bytes_token)\n\n    #                 tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (length_bytes_token))\n    #                 token_count[tuple_bytes_token] = token_count_get(tuple_bytes_token, 0) + 1\n    #         else:\n    #             for token in doc.split():\n    #                 bytes_token = token.encode(\"utf-8\")\n\n    #                 tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n    #                 token_count[tuple_bytes_token] = token_count_get(tuple_bytes_token, 0) + 1\n\n    #         # if gpt2_regex:\n    #         #     # PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n    #         #     pre_tokens = PAT.finditer(doc)\n    #         #     pre_tokens = [match.group(0) for match in pre_tokens]\n    #         # else:\n    #         #     pre_tokens = doc.split()\n    \n    #         # for token in pre_tokens:\n    #         #     bytes_token = token.encode(\"utf-8\")\n                \n    #         #     tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n    #         #     token_count[tuple_bytes_token] = token_count_get(tuple_bytes_token, 0) + 1\n            \n    #     return token_count\n    \n    @staticmethod\n    def pretokenize_and_count(docs: list[str], gpt2_regex: bool = False) -> dict[tuple[bytes], int]:\n        token_count : dict[tuple[bytes], int] = {}\n        token_count_get = token_count.get\n        \n        # Build cache mapping pretoken bytes format to tuple byte format \n        cache :dict[bytes, tuple[bytes]] = {}\n        cache_get = cache.get\n    \n        for doc in docs:\n            # pre_tokens = None\n            # Use a regex-based pre-tokenizer (used by GPT-2; Radford et al., 2019)\n            if gpt2_regex:\n                for token in PAT.finditer(doc):\n                    token_str = token.group(0)\n                    bytes_token = token_str.encode(\"utf-8\")\n\n                    pretoken_tuplebytes = cache_get(bytes_token)\n\n                    if pretoken_tuplebytes is None:\n                        # Build cache\n                        length_bytes_token = len(bytes_token)\n                        pretoken_tuplebytes = tuple(bytes_token[i : i+1] for i in range (length_bytes_token))\n                        cache[bytes_token] = pretoken_tuplebytes\n                                        \n                    token_count[pretoken_tuplebytes] = token_count_get(pretoken_tuplebytes, 0) + 1\n            else:\n                for token in doc.split():\n                    bytes_token = token.encode(\"utf-8\")\n\n                    tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n                    token_count[tuple_bytes_token] = token_count_get(tuple_bytes_token, 0) + 1\n\n            # if gpt2_regex:\n            #     # PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n            #     pre_tokens = PAT.finditer(doc)\n            #     pre_tokens = [match.group(0) for match in pre_tokens]\n            # else:\n            #     pre_tokens = doc.split()\n    \n            # for token in pre_tokens:\n            #     bytes_token = token.encode(\"utf-8\")\n                \n            #     tuple_bytes_token = tuple(bytes_token[i : i+1] for i in range (len(bytes_token)))\n            #     token_count[tuple_bytes_token] = token_count_get(tuple_bytes_token, 0) + 1\n            \n        return token_count\n    \n    def pretokenize_and_count_task(path: str, start: int, end :int, special_token: list[str], gpt2_regex: bool) -> dict[tuple[bytes], int]:\n        # Get chunk\n        with open(path, \"rb\") as f:\n            f.seek(start)\n            chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n        \n        # Remove special token\n        docs = Tokenizer.remove_special_tokens_static(chunk, special_token)\n\n        # Build pretoken counts dict\n        pretoken_counts = Tokenizer.pretokenize_and_count(docs, gpt2_regex)\n\n        return pretoken_counts\n    \n    def pretokenize(self, input_path: str, gpt2_regex: bool) -> dict[tuple[bytes], int]:\n        # Read training data\n        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n    \n        # Removing special tokens\n        docs = self.remove_special_tokens(text)\n\n        # Pre-tokenization\n        pretokens = self.pretokenize_and_count(docs, gpt2_regex)\n\n        return pretokens\n    \n    def pretokenize_parallel(self, path: str, gpt2_regex: bool) -> dict[tuple[bytes], int]:\n        # Get logical core number\n        core_num = os.cpu_count()\n\n        # Get boundaries of chunks\n        # TODO: special token should not hardcode\n        with open(path, \"rb\") as f:\n            boundaries = Tokenizer.find_chunk_boundaries(\n                f, core_num, \"<|endoftext|>\".encode(\"utf-8\"))\n        \n        # Parallel pretoken\n        with ProcessPoolExecutor(max_workers=core_num) as executor:\n            futures = [executor.submit(Tokenizer.pretokenize_and_count_task, path, start, end, self.special_tokens, gpt2_regex) for start, end in zip(boundaries[:-1], boundaries[1:])]\n        \n        pretoken_counts = {}\n        pretoken_counts_get = pretoken_counts.get\n\n        for future in as_completed(futures):\n            for pretoken, count in future.result().items():\n                pretoken_counts[pretoken] = pretoken_counts_get(pretoken, 0) + count\n        \n        return pretoken_counts\n  \n    # @staticmethod\n    # def build_paircount_and_cache(\n    #     pretokens : dict[tuple[bytes, ...], int]\n    # ) -> tuple[\n    #     dict[tuple[bytes], int], \n    #     dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]]\n    #     ]:\n    \n    #     pair_count: dict[tuple[bytes], int] = {}\n    #     cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]] = defaultdict(set)\n    \n    #     for k, v in pretokens.items():\n    #         for i in range(len(k)-1):\n    #             pair_count[k[i : i+2]] = pair_count.get(k[i : i+2], 0) + v\n    \n    #             cache[k[i : i+2]].add((k, v))\n    \n    #     return pair_count, cache\n\n\n    # @staticmethod\n    # def build_paircount_and_cache(\n    #     pretokens : dict[tuple[bytes, ...], int]\n    # ) -> tuple[\n    #     dict[tuple[bytes], int], \n    #     dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]]\n    #     ]:\n    \n    #     pair_count: dict[tuple[bytes], int] = {}\n    #     cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]] = defaultdict(set)\n    \n    #     for k, v in pretokens.items():\n    #         for i in range(len(k)-1):\n    #             pair = k[i : i+2]\n\n    #             pair_count[pair] = pair_count.get(pair, 0) + v\n    \n    #             cache[pair].add((k, v))\n    \n    #     return pair_count, cache\n    \n    # @staticmethod\n    # def build_paircount_and_cache(\n    #     pretokens : dict[tuple[bytes, ...], int]\n    # ) -> tuple[\n    #     dict[tuple[bytes], int], \n    #     dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]]\n    #     ]:\n    \n    #     pair_count: dict[tuple[bytes], int] = {}\n    #     cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]] = defaultdict(set)\n\n    #     pc_get = pair_count.get\n    \n    #     for k, v in pretokens.items():\n    #         for i in range(len(k)-1):\n    #             pair = k[i : i+2]\n\n    #             # pair_count[pair] = pair_count.get(pair, 0) + v\n    #             pair_count[pair] = pc_get(pair, 0) + v\n\n    #             cache[pair].add((k, v))\n    \n    #     return pair_count, cache\n    \n    @staticmethod\n    def build_paircount_and_cache(\n        pretokens : dict[tuple[bytes, ...], int]\n    ) -> tuple[\n        dict[tuple[bytes], int], \n        dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]]\n        ]:\n    \n        pair_count: dict[tuple[bytes], int] = {}\n        cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]] = defaultdict(set)\n\n        pc_get = pair_count.get\n    \n        for k, v in pretokens.items():\n            length_k = len(k)-1\n            for i in range(length_k):\n                pair = k[i : i+2]\n\n                # pair_count[pair] = pair_count.get(pair, 0) + v\n                pair_count[pair] = pc_get(pair, 0) + v\n\n                cache[pair].add((k, v))\n    \n        return pair_count, cache\n    \n    def build_heap(self, pair_count: dict[tuple[bytes], int]):\n        # Prepare for heapify\n        self._heap = [(-count, _Desc(pair)) for pair, count in pair_count.items()]\n\n        heapq.heapify(self._heap)\n    \n    def update_heap(self, changed_paircount: dict[tuple[bytes], int]):\n        for pair, count in changed_paircount.items():\n            heapq.heappush(self._heap, (-count, _Desc(pair)))\n    \n    # @staticmethod\n    # def _pick_best_mergetoken(pair_count: dict[tuple[bytes], int]) -> tuple[tuple[bytes], int]:\n    #     try:\n    #         return max(\n    #             pair_count.items(),\n    #             key = lambda kv: (kv[1], kv[0])\n    #         )\n    #     except Exception as e:\n\n    #     # Log or print the freqs that caused the failure\n    #         print(\"Error picking best token, pair_count was:\", pair_count)\n    #         raise\n    \n    def _pick_best_mergetoken(self, pair_count: dict[tuple[bytes], int]) -> tuple[tuple[bytes], int]:\n        while len(self._heap):\n            best_heap = self._heap[0]\n            pair = best_heap[1].x\n            count = -best_heap[0]\n\n            if pair_count.get(pair, 0) == count:\n                return (pair, count)\n            else:\n                heapq.heappop(self._heap)\n    \n    # @staticmethod\n    # def _build_new_pretoken(\n    #     old_pretoken: tuple[tuple[bytes, ...], int], \n    #     best_paircount: tuple[bytes, ...]\n    #     ) ->  tuple[tuple[bytes, ...], int]:\n    \n    #     new_pretoken_pair = ()\n    #     old_pretoken_pair = old_pretoken[0]\n    #     best_pair = best_paircount\n    #     i = 0\n    \n    #     while i < len(old_pretoken_pair)-1:\n    #         if old_pretoken_pair[i : i+2] == best_pair:\n    #             new_pretoken_pair = new_pretoken_pair + (old_pretoken_pair[i] + old_pretoken_pair[i+1],)\n    \n    #             if i == len(old_pretoken_pair)-3:\n    #                 new_pretoken_pair = new_pretoken_pair + (old_pretoken_pair[i+2],)\n    \n    #             i = i+2\n    #         else:\n    #             new_pretoken_pair = new_pretoken_pair + (old_pretoken_pair[i],)\n    \n    #             if i == len(old_pretoken_pair)-2:\n    #                 new_pretoken_pair = new_pretoken_pair + (old_pretoken_pair[i+1],)\n    \n    #             i = i+1\n        \n    #     new_pretoken = (new_pretoken_pair, old_pretoken[1])\n    \n    #     return new_pretoken\n    \n    @staticmethod\n    def _build_new_pretoken(\n        old_pretoken: tuple[tuple[bytes, ...], int], \n        best_paircount: tuple[bytes, ...]\n        ) ->  tuple[tuple[bytes, ...], int]:\n    \n        # new_pretoken_pair = ()\n        new_pretoken_pair: list[bytes] = []\n\n        old_pretoken_pair = old_pretoken[0]\n        best_pair = best_paircount\n\n        i = 0\n        L = len(old_pretoken_pair) - 1\n    \n        while i < L:\n            if old_pretoken_pair[i : i+2] == best_pair:\n                # new_pretoken_pair = new_pretoken_pair + (old_pretoken_pair[i] + old_pretoken_pair[i+1],)\n                new_pretoken_pair.append(old_pretoken_pair[i] + old_pretoken_pair[i+1])\n    \n                # if i == len(old_pretoken_pair)-3:\n                #     new_pretoken_pair = new_pretoken_pair + (old_pretoken_pair[i+2],)\n    \n                i = i+2\n            else:\n                # new_pretoken_pair = new_pretoken_pair + (old_pretoken_pair[i],)\n                new_pretoken_pair.append(old_pretoken_pair[i])\n    \n                # if i == len(old_pretoken_pair)-2:\n                #     new_pretoken_pair = new_pretoken_pair + (old_pretoken_pair[i+1],)\n    \n                i = i+1\n            \n            if i == L:\n                new_pretoken_pair.append(old_pretoken_pair[i])\n        \n        new_pretoken = (tuple(new_pretoken_pair), old_pretoken[1])\n    \n        return new_pretoken\n    \n    # @staticmethod\n    # def _delete_old_contribution(\n    #     pretoken: tuple[tuple[bytes, ...], int], \n    #     pair_count: dict[tuple[bytes], int], \n    #     reversed_cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]]\n    #     ) -> tuple[dict[tuple[bytes], int], dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]]]:\n\n    #     pretoken_pair = pretoken[0]\n    #     pretoken_count = pretoken[1]\n    \n    #     for i in range (len(pretoken_pair)-1):\n    #         pair = pretoken_pair[i : i+2]\n    \n    #         pair_count[pair] = pair_count[pair] - pretoken_count\n    #         if pair_count[pair] == 0:\n    #             del pair_count[pair]\n    \n    #         reversed_cache[pair].discard(pretoken)\n    #         if not reversed_cache[pair]:\n    #             del reversed_cache[pair]\n        \n    #     return pair_count, reversed_cache\n    \n    # @staticmethod\n    # def _delete_old_contribution(\n    #     pretoken: tuple[tuple[bytes, ...], int], \n    #     pair_count: dict[tuple[bytes], int], \n    #     reversed_cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]],\n    #     changed_paircount: dict[tuple[bytes], int]\n    #     ) -> tuple[dict[tuple[bytes], int], dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]], dict[tuple[bytes], int]]:\n\n    #     pretoken_pair = pretoken[0]\n    #     pretoken_count = pretoken[1]\n\n    #     changed_paircount_get = changed_paircount.get\n    \n    #     for i in range (len(pretoken_pair)-1):\n    #         pair = pretoken_pair[i : i+2]\n    \n    #         pair_count[pair] = pair_count[pair] - pretoken_count\n            \n    #         # Record negative change \n    #         changed_paircount[pair] = changed_paircount_get(pair, 0) - pretoken_count\n\n    #         if pair_count[pair] == 0:\n    #             del pair_count[pair]\n    \n    #         reversed_cache[pair].discard(pretoken)\n    #         if not reversed_cache[pair]:\n    #             del reversed_cache[pair]\n        \n    #     return pair_count, reversed_cache, changed_paircount\n    \n    @staticmethod\n    def _delete_old_contribution(\n        pretoken: tuple[tuple[bytes, ...], int], \n        pair_count: dict[tuple[bytes], int], \n        reversed_cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]],\n        changed_paircount: dict[tuple[bytes], int]\n        ) -> tuple[dict[tuple[bytes], int], dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]], dict[tuple[bytes], int]]:\n\n        pretoken_pair = pretoken[0]\n        pretoken_count = pretoken[1]\n\n        changed_paircount_get = changed_paircount.get\n        length_pretoken_pair = len(pretoken_pair)-1\n    \n        for i in range (length_pretoken_pair):\n            pair = pretoken_pair[i : i+2]\n    \n            pair_count[pair] = pair_count[pair] - pretoken_count\n            \n            # Record negative change \n            changed_paircount[pair] = changed_paircount_get(pair, 0) - pretoken_count\n\n            if pair_count[pair] == 0:\n                del pair_count[pair]\n    \n            reversed_cache[pair].discard(pretoken)\n            if not reversed_cache[pair]:\n                del reversed_cache[pair]\n        \n        return pair_count, reversed_cache, changed_paircount\n    \n    # @staticmethod\n    # def _add_new_contribution(\n    #     pretoken: tuple[tuple[bytes, ...], int], \n    #     pair_count: dict[tuple[bytes], int], \n    #     reversed_cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]]\n    #     ) -> tuple[dict[tuple[bytes], int], dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]]]:\n\n    #     reversed_cache = defaultdict(set, reversed_cache)\n    #     pretoken_pair = pretoken[0]\n    #     pretoken_count = pretoken[1]\n    \n    #     for i in range (len(pretoken_pair)-1):\n    #         pair = pretoken_pair[i : i+2]\n    \n    #         pair_count[pair] = pair_count.get(pair, 0) + pretoken_count\n    \n    #         reversed_cache[pair].add(pretoken)\n        \n    #     return pair_count, reversed_cache\n    \n    @staticmethod\n    def _add_new_contribution(\n        pretoken: tuple[tuple[bytes, ...], int], \n        pair_count: dict[tuple[bytes], int], \n        reversed_cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]],\n        changed_paircount: dict[tuple[bytes], int]\n        ) -> tuple[dict[tuple[bytes], int], dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]], dict[tuple[bytes], int]]:\n\n        reversed_cache = defaultdict(set, reversed_cache)\n        pretoken_pair = pretoken[0]\n        pretoken_count = pretoken[1]\n\n        pair_count_get = pair_count.get\n        changed_paircount_get = changed_paircount.get\n\n        length_pretoken_pair = len(pretoken_pair)-1\n    \n        for i in range (length_pretoken_pair):\n            pair = pretoken_pair[i : i+2]\n    \n            pair_count[pair] = pair_count_get(pair, 0) + pretoken_count\n\n            # Record positive change\n            changed_paircount[pair] = changed_paircount_get(pair, 0) + pretoken_count\n    \n            reversed_cache[pair].add(pretoken)\n        \n        return pair_count, reversed_cache, changed_paircount\n\n    # @staticmethod\n    # def merge_new(\n    #     pair_counts: dict[tuple[bytes], int], \n    #     reversed_cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]],\n    #     best_pair: tuple[bytes, ...]\n    # ) -> tuple[dict[tuple[bytes], int], dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]]]:\n\n    #     affected_pretokens = reversed_cache[best_pair].copy()\n    \n    #     for old_pretoken in affected_pretokens:\n    #         new_pretoken = Tokenizer._build_new_pretoken(old_pretoken, best_pair)\n    \n    #         # Update, delete old pretoken contribution\n    #         pair_counts, reversed_cache = Tokenizer._delete_old_contribution(old_pretoken, pair_counts, reversed_cache)\n    #         # update, add new pretoken contrbution\n    #         pair_counts, reversed_cache = Tokenizer._add_new_contribution(new_pretoken, pair_counts, reversed_cache)\n\n    #     return pair_counts, reversed_cache\n    \n    @staticmethod\n    def merge_new(\n        pair_counts: dict[tuple[bytes], int], \n        reversed_cache: dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]],\n        best_pair: tuple[bytes, ...]\n    ) -> tuple[dict[tuple[bytes], int], dict[tuple[bytes, ...], set[tuple[tuple[bytes, ...], int]]], dict[tuple[bytes], int]]:\n\n        affected_pretokens = reversed_cache[best_pair].copy()\n\n        delta_changed_paircount: dict[tuple[bytes], int] = {}\n    \n        for old_pretoken in affected_pretokens:\n            new_pretoken = Tokenizer._build_new_pretoken(old_pretoken, best_pair)\n    \n            # Update, delete old pretoken contribution\n            pair_counts, reversed_cache, delta_changed_paircount = Tokenizer._delete_old_contribution(old_pretoken, pair_counts, reversed_cache, delta_changed_paircount)\n            # Update, add new pretoken contrbution\n            pair_counts, reversed_cache, delta_changed_paircount = Tokenizer._add_new_contribution(new_pretoken, pair_counts, reversed_cache, delta_changed_paircount)\n        \n        # Build changed pair count dict\n        changed_paircount = {}\n\n        for changed_pair, changed_count in delta_changed_paircount.items():\n            # If the changed count is zero, which means no changed, should not include here\n            if changed_count and changed_pair in pair_counts:\n            # if changed_count and pair_counts.get(changed_pair, 0) != 0:\n                changed_paircount[changed_pair] = pair_counts[changed_pair]\n\n        return pair_counts, reversed_cache, changed_paircount\n    \n    \n    def update_vocab(self, best_pair: tuple[tuple[bytes], int]):\n        # sorted_vocab = sorted(self.vocab.items(), reverse=True)\n        # new_index =  sorted_vocab[0][0] + 1\n        \n        k = best_pair[0]\n        k = k[0] + k[1]\n    \n        self.vocab[self.next_id] = k\n\n        self.next_id += 1\n    \n    def _save_vocabulary_merges(self):\n        \"\"\"\n        Serialize the resulting vocabulary and merges to disk for further inspection\n\n        Args:\n\n        Returns:\n        \"\"\"\n        vocab_serialized = {\n            str(token_id): vocab_bytes.decode(\"utf-8\", \"replace\")\n            for token_id, vocab_bytes in self.vocab.items()\n        }\n\n        merge_serialized = [\n            [first_bytes.decode(\"utf-8\", \"replace\"), second_bytes.decode(\"utf-8\", \"replace\")]\n            for first_bytes, second_bytes in self.merge\n        ]\n\n        with open(self.serialization_vocab_path, 'w', encoding=\"utf-8\") as f:\n            json.dump(vocab_serialized, f, indent=2, ensure_ascii=False)\n        \n        with open(self.serialization_merge_path, 'w', encoding=\"utf-8\") as f:\n            json.dump(merge_serialized, f, indent=2, ensure_ascii=False)\n    \n    # def train_bpe(self, input_path: str, vocab_size: int, gpt2_regex: bool, enable_parallel: bool) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n    #     # Init vocab\n    #     self.init_vocab()\n\n    #     if enable_parallel:\n    #         pretokens = self.pretokenize_parallel(input_path, gpt2_regex)\n    #     else:\n    #         pretokens = self.pretokenize(input_path, gpt2_regex)\n      \n    #     # Build the first pair count and cache(pair to corresponding pretokens)\n    #     pair_counts, reversed_cache = self.build_paircount_and_cache(pretokens)\n    \n    #     for i in range(vocab_size - 256 - len(self.special_tokens)):\n    #         # Pick best adjcent tokens to merge\n    #         best_pair = self._pick_best_mergetoken(pair_counts)\n    \n    #         # Log pair counts, best pair and step\n    #         self.dump_pair_count(pair_counts, best_pair, i)\n    \n    #         # Update pair counts and cache\n    #         pair_counts,  reversed_cache = self.merge_new(pair_counts, reversed_cache, best_pair[0])\n    \n    #         # TODO: optimize point, insert vocab and merges two times\n    #         # Update vocabs\n    #         self.update_vocab(best_pair)\n    #         # Update merges\n    #         self.merge.append((best_pair[0][0], best_pair[0][1]))\n        \n    #     return self.vocab, self.merge\n\n    def train_bpe(self, input_path: str, vocab_size: int, gpt2_regex: bool, enable_parallel: bool) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n        # Init vocab\n        self.init_vocab()\n\n        if enable_parallel:\n            pretokens = self.pretokenize_parallel(input_path, gpt2_regex)\n        else:\n            pretokens = self.pretokenize(input_path, gpt2_regex)\n      \n        # Build the first pair count and cache(pair to corresponding pretokens)\n        pair_counts, reversed_cache = self.build_paircount_and_cache(pretokens)\n\n        self.build_heap(pair_counts)\n\n        merge_size = vocab_size - 256 - len(self.special_tokens)\n    \n        for i in range(merge_size):\n            # Pick best adjcent tokens to merge\n            best_pair = self._pick_best_mergetoken(pair_counts)\n    \n            # Log pair counts, best pair and step\n            self.dump_pair_count(pair_counts, best_pair, i)\n    \n            # Update pair counts and cache\n            pair_counts, reversed_cache, changed_paircount = self.merge_new(pair_counts, reversed_cache, best_pair[0])\n\n            self.update_heap(changed_paircount)\n    \n            # TODO: optimize point, insert vocab and merges two times\n            # Update vocabs\n            self.update_vocab(best_pair)\n            # Update merges\n            self.merge.append((best_pair[0][0], best_pair[0][1]))\n        \n        if self.serialization:\n            self._save_vocabulary_merges()\n        \n        return self.vocab, self.merge","metadata":{"id":"dfLZ33ChBw6p","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:23:00.706596Z","iopub.execute_input":"2025-08-27T12:23:00.707542Z","iopub.status.idle":"2025-08-27T12:23:00.728928Z","shell.execute_reply.started":"2025-08-27T12:23:00.707512Z","shell.execute_reply":"2025-08-27T12:23:00.727960Z"}},"outputs":[{"name":"stdout","text":"Overwriting tokenizer.py\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"Train bpe","metadata":{"id":"9uiRP5_EC1Wd"}},{"cell_type":"code","source":"from tokenizer import Tokenizer\nimport time\nimport tracemalloc\nimport os\nimport psutil\nimport contextlib\nimport yaml\n\n@contextlib.contextmanager\ndef perf_monitor(enabled: bool = True):\n    if not enabled:\n        yield {}\n        return\n    \n   # Stat time and memory\n    tracemalloc.start()\n    start_time = time.perf_counter()\n    \n    try:\n        yield {}\n    finally:\n        # Stat time and memory\n        end_time = time.perf_counter()\n        _, peak = tracemalloc.get_traced_memory()\n        tracemalloc.stop()\n\n        # Memory stat(rss)\n        process = psutil.Process(os.getpid())\n        rss_mem = process.memory_info().rss / (1024 * 1024)\n\n        # Build report\n        report = f\"\"\"\n        Performence report\n        -------------------------------\n        Total time                      :{(end_time - start_time):.2f} seconds\n        Peak memory managed by python   :{peak / 1024 / 1024:.2f} MB\n        Total physical memory used(RSS) :{rss_mem:.2f} MB\n        \"\"\"\n\n        print(report)\n\ndef main():\n    with perf_monitor(enabled=False):\n\n        with open(\"/kaggle/working/config_kaggle.yaml\", \"r\") as f:\n            config = yaml.safe_load(f)\n\n        # Init tokenizer\n        tokenizer = Tokenizer(\n            config[\"special_tokens\"], \n            enable_log=config[\"enable_log\"], \n            log_path=config[\"log_path\"],\n            serialization=config[\"serialization\"],\n            serialization_vocab_path= config[\"serialization_vocab_path\"],\n            serialization_merge_path= config[\"serialization_merge_path\"]\n        )\n    \n        # Training\n        vocab, merges = tokenizer.train_bpe(\n            config[\"traindata_path\"], \n            vocab_size=config[\"vocab_size\"], \n            gpt2_regex=config[\"gpt2_regex\"], \n            enable_parallel=config[\"parallel\"]\n        )\n\n    # Build report\n    report = f\"\"\"\n        BPE Tokenizer Training report\n        -------------------------------\n        Vocabuary size                  :{len(vocab)}\n        Number of merges                :{len(merges)}\n        First 5 merges                  :{merges[:5]}\n    \"\"\"\n\n    print(report)\n\n# if __name__ == \"__main__\":\n#     main()\n\n# Profile the function inline\n%prun -s cumtime main()","metadata":{"id":"BNTenAf6C4Z9","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:32:29.153789Z","iopub.execute_input":"2025-08-27T12:32:29.155056Z","iopub.status.idle":"2025-08-27T12:46:19.691456Z","shell.execute_reply.started":"2025-08-27T12:32:29.155012Z","shell.execute_reply":"2025-08-27T12:46:19.690288Z"}},"outputs":[{"name":"stdout","text":"\n        BPE Tokenizer Training report\n        -------------------------------\n        Vocabuary size                  :10000\n        Number of merges                :9743\n        First 5 merges                  :[(b' ', b't'), (b'h', b'e'), (b' ', b'a'), (b' ', b's'), (b' ', b'w')]\n    \n ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         10959364 function calls (10910628 primitive calls) in 830.515 seconds\n\n   Ordered by: cumulative time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000  830.515  830.515 {built-in method builtins.exec}\n        1    0.064    0.064  830.515  830.515 <string>:1(<module>)\n        1    0.040    0.040  830.451  830.451 4062424040.py:42(main)\n        1   11.406   11.406  830.406  830.406 tokenizer.py:823(train_bpe)\n        1    0.141    0.141  535.781  535.781 tokenizer.py:357(pretokenize_parallel)\n       66  535.472    8.113  535.472    8.113 {method 'acquire' of '_thread.lock' objects}\n        1    0.000    0.000  535.464  535.464 _base.py:646(__exit__)\n        1    0.000    0.000  535.464  535.464 process.py:842(shutdown)\n       26    0.000    0.000  535.464   20.595 threading.py:1125(_wait_for_tstate_lock)\n        1    0.000    0.000  535.464  535.464 threading.py:1087(join)\n     9743   85.004    0.009  279.606    0.029 tokenizer.py:727(merge_new)\n   277780  182.600    0.001  184.876    0.001 tokenizer.py:679(_add_new_contribution)\n   277780    4.979    0.000    6.846    0.000 tokenizer.py:628(_delete_old_contribution)\n   277780    2.407    0.000    2.824    0.000 tokenizer.py:535(_build_new_pretoken)\n  4104120    2.710    0.000    2.710    0.000 {method 'get' of 'dict' objects}\n        1    1.365    1.365    2.081    2.081 tokenizer.py:445(build_paircount_and_cache)\n  1456961    1.149    0.000    1.149    0.000 {method 'add' of 'set' objects}\n  1362724    1.016    0.000    1.016    0.000 {method 'discard' of 'set' objects}\n     9743    0.610    0.000    0.724    0.000 tokenizer.py:476(update_heap)\n     9743    0.142    0.000    0.549    0.000 tokenizer.py:493(_pick_best_mergetoken)\n    42746    0.301    0.000    0.337    0.000 {built-in method _heapq.heappop}\n  1369407    0.328    0.000    0.328    0.000 {method 'append' of 'list' objects}\n   945942    0.282    0.000    0.282    0.000 {built-in method builtins.len}\n        1    0.000    0.000    0.183    0.183 tokenizer.py:769(_save_vocabulary_merges)\n        2    0.030    0.015    0.137    0.068 __init__.py:120(dump)\n    88722    0.020    0.000    0.095    0.000 encoder.py:414(_iterencode)\n   122104    0.078    0.000    0.081    0.000 {built-in method _heapq.heappush}\n97433/48718    0.048    0.000    0.058    0.000 encoder.py:278(_iterencode_list)\n     9743    0.054    0.000    0.054    0.000 tokenizer.py:758(update_vocab)\n     9743    0.048    0.000    0.048    0.000 {method 'copy' of 'set' objects}\n    43440    0.039    0.000    0.039    0.000 tokenizer.py:20(__lt__)\n        1    0.000    0.000    0.038    0.038 tokenizer.py:369(<listcomp>)\n        4    0.000    0.000    0.038    0.009 process.py:788(submit)\n        4    0.000    0.000    0.036    0.009 process.py:744(_start_executor_manager_thread)\n        1    0.032    0.032    0.036    0.036 tokenizer.py:782(<listcomp>)\n        1    0.000    0.000    0.033    0.033 process.py:769(_launch_processes)\n        4    0.000    0.000    0.033    0.008 process.py:777(_spawn_process)\n        4    0.000    0.000    0.032    0.008 process.py:110(start)\n        4    0.000    0.000    0.032    0.008 context.py:278(_Popen)\n        4    0.000    0.000    0.031    0.008 popen_fork.py:15(__init__)\n   124212    0.027    0.000    0.027    0.000 tokenizer.py:17(__init__)\n        4    0.001    0.000    0.022    0.005 popen_fork.py:62(_launch)\n        4    0.020    0.005    0.020    0.005 {built-in method posix.fork}\n    40004    0.012    0.000    0.017    0.000 encoder.py:334(_iterencode_dict)\n    19494    0.016    0.000    0.016    0.000 {method 'items' of 'dict' objects}\n    88720    0.011    0.000    0.011    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n        1    0.000    0.000    0.010    0.010 tokenizer.py:470(build_heap)\n        1    0.008    0.008    0.010    0.010 tokenizer.py:777(<dictcomp>)\n        1    0.008    0.008    0.009    0.009 tokenizer.py:472(<listcomp>)\n        4    0.000    0.000    0.009    0.002 util.py:436(_flush_std_streams)\n        8    0.000    0.000    0.009    0.001 iostream.py:471(flush)\n        9    0.000    0.000    0.009    0.001 threading.py:611(wait)\n        9    0.000    0.000    0.008    0.001 threading.py:295(wait)\n    78820    0.008    0.000    0.008    0.000 {built-in method builtins.isinstance}\n     9743    0.007    0.000    0.007    0.000 tokenizer.py:63(dump_pair_count)\n    39486    0.006    0.000    0.006    0.000 {built-in method _json.encode_basestring}\n    29486    0.006    0.000    0.006    0.000 {method 'decode' of 'bytes' objects}\n        1    0.000    0.000    0.003    0.003 __init__.py:117(safe_load)\n        1    0.000    0.000    0.003    0.003 __init__.py:74(load)\n        1    0.000    0.000    0.003    0.003 constructor.py:47(get_single_data)\n        1    0.000    0.000    0.003    0.003 composer.py:29(get_single_node)\n        1    0.000    0.000    0.003    0.003 composer.py:50(compose_document)\n     22/1    0.000    0.000    0.003    0.003 composer.py:63(compose_node)\n       62    0.000    0.000    0.003    0.000 parser.py:94(check_event)\n        1    0.000    0.000    0.003    0.003 composer.py:117(compose_mapping_node)\n        1    0.000    0.000    0.002    0.002 threading.py:945(start)\n      165    0.000    0.000    0.002    0.000 scanner.py:113(check_token)\n       17    0.000    0.000    0.002    0.000 iostream.py:202(schedule)\n        1    0.000    0.000    0.001    0.001 process.py:634(__init__)\n       32    0.000    0.000    0.001    0.000 scanner.py:156(fetch_more_tokens)\n       17    0.001    0.000    0.001    0.000 socket.py:545(send)\n     9751    0.001    0.000    0.001    0.000 {built-in method builtins.id}\n        1    0.001    0.001    0.001    0.001 {built-in method _heapq.heapify}\n       11    0.000    0.000    0.001    0.000 parser.py:427(parse_block_mapping_key)\n        5    0.001    0.000    0.001    0.000 synchronize.py:50(__init__)\n       10    0.000    0.000    0.001    0.000 parser.py:446(parse_block_mapping_value)\n        4    0.001    0.000    0.001    0.000 {built-in method io.open}\n        1    0.000    0.000    0.001    0.001 process.py:168(__init__)\n        1    0.000    0.000    0.001    0.001 queues.py:37(__init__)\n        4    0.000    0.000    0.001    0.000 context.py:65(Lock)\n        4    0.000    0.000    0.001    0.000 synchronize.py:168(__init__)\n        4    0.000    0.000    0.001    0.000 process.py:80(__init__)\n        4    0.001    0.000    0.001    0.000 util.py:189(__init__)\n       15    0.000    0.000    0.001    0.000 scanner.py:668(fetch_plain)\n       22    0.000    0.000    0.001    0.000 parser.py:273(parse_node)\n       15    0.000    0.000    0.001    0.000 scanner.py:1270(scan_plain)\n       20    0.000    0.000    0.001    0.000 parser.py:270(parse_block_node_or_indentless_sequence)\n        5    0.000    0.000    0.001    0.000 process.py:82(wakeup)\n        1    0.000    0.000    0.000    0.000 process.py:291(__init__)\n       10    0.000    0.000    0.000    0.000 threading.py:562(__init__)\n        5    0.000    0.000    0.000    0.000 connection.py:182(send_bytes)\n      270    0.000    0.000    0.000    0.000 scanner.py:145(need_more_tokens)\n        1    0.000    0.000    0.000    0.000 contextlib.py:299(helper)\n       19    0.000    0.000    0.000    0.000 threading.py:243(__init__)\n        1    0.000    0.000    0.000    0.000 context.py:110(SimpleQueue)\n        1    0.000    0.000    0.000    0.000 threading.py:856(__init__)\n        5    0.000    0.000    0.000    0.000 _base.py:199(as_completed)\n        1    0.000    0.000    0.000    0.000 queues.py:339(__init__)\n        1    0.000    0.000    0.000    0.000 tokenizer.py:88(find_chunk_boundaries)\n        1    0.000    0.000    0.000    0.000 loader.py:33(__init__)\n       79    0.000    0.000    0.000    0.000 reader.py:99(forward)\n        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n        5    0.000    0.000    0.000    0.000 connection.py:406(_send_bytes)\n        5    0.000    0.000    0.000    0.000 scanner.py:654(fetch_double)\n       23    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n        2    0.000    0.000    0.000    0.000 iostream.py:526(write)\n        5    0.000    0.000    0.000    0.000 scanner.py:657(fetch_flow_scalar)\n        4    0.000    0.000    0.000    0.000 __init__.py:237(_releaseLock)\n        1    0.000    0.000    0.000    0.000 constructor.py:54(construct_document)\n        5    0.000    0.000    0.000    0.000 synchronize.py:121(_make_name)\n        1    0.000    0.000    0.000    0.000 composer.py:99(compose_sequence_node)\n        4    0.000    0.000    0.000    0.000 queue.py:122(put)\n        5    0.000    0.000    0.000    0.000 scanner.py:1134(scan_flow_scalar)\n        1    0.000    0.000    0.000    0.000 parser.py:139(parse_implicit_document_start)\n       25    0.000    0.000    0.000    0.000 threading.py:1192(is_alive)\n        5    0.000    0.000    0.000    0.000 _base.py:177(_yield_finished_futures)\n        2    0.000    0.000    0.000    0.000 constructor.py:410(construct_yaml_map)\n        5    0.000    0.000    0.000    0.000 {method 'seek' of '_io.BufferedReader' objects}\n        1    0.000    0.000    0.000    0.000 context.py:85(BoundedSemaphore)\n        2    0.000    0.000    0.000    0.000 parser.py:381(parse_block_sequence_entry)\n        1    0.000    0.000    0.000    0.000 constructor.py:215(construct_mapping)\n        1    0.000    0.000    0.000    0.000 parser.py:376(parse_block_sequence_first_entry)\n        5    0.000    0.000    0.000    0.000 tempfile.py:293(__next__)\n       32    0.000    0.000    0.000    0.000 scanner.py:752(scan_to_next_token)\n        1    0.000    0.000    0.000    0.000 synchronize.py:151(__init__)\n        1    0.000    0.000    0.000    0.000 reader.py:59(__init__)\n        1    0.000    0.000    0.000    0.000 constructor.py:132(construct_mapping)\n        2    0.000    0.000    0.000    0.000 iostream.py:456(_schedule_flush)\n      273    0.000    0.000    0.000    0.000 scanner.py:279(stale_possible_simple_keys)\n        5    0.000    0.000    0.000    0.000 scanner.py:1185(scan_flow_scalar_non_spaces)\n        5    0.000    0.000    0.000    0.000 _weakrefset.py:85(add)\n       20    0.000    0.000    0.000    0.000 composer.py:88(compose_scalar_node)\n      560    0.000    0.000    0.000    0.000 reader.py:87(peek)\n        4    0.000    0.000    0.000    0.000 process.py:61(_cleanup)\n       47    0.000    0.000    0.000    0.000 scanner.py:135(get_token)\n       10    0.000    0.000    0.000    0.000 scanner.py:545(fetch_value)\n        1    0.000    0.000    0.000    0.000 {built-in method _thread.start_new_thread}\n       78    0.000    0.000    0.000    0.000 reader.py:114(get_mark)\n       11    0.000    0.000    0.000    0.000 {built-in method posix.pipe}\n        1    0.000    0.000    0.000    0.000 reader.py:122(determine_encoding)\n       22    0.000    0.000    0.000    0.000 constructor.py:67(construct_object)\n       18    0.000    0.000    0.000    0.000 {built-in method posix.close}\n        6    0.000    0.000    0.000    0.000 util.py:174(register_after_fork)\n        5    0.000    0.000    0.000    0.000 random.py:480(choices)\n        4    0.000    0.000    0.000    0.000 _base.py:328(__init__)\n        6    0.000    0.000    0.000    0.000 popen_fork.py:24(poll)\n        4    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n       15    0.000    0.000    0.000    0.000 scanner.py:1311(scan_plain_spaces)\n        5    0.000    0.000    0.000    0.000 connection.py:381(_send)\n        2    0.000    0.000    0.000    0.000 parser.py:264(parse_block_node)\n       10    0.000    0.000    0.000    0.000 threading.py:1453(current_thread)\n        1    0.000    0.000    0.000    0.000 parser.py:422(parse_block_mapping_first_key)\n        2    0.000    0.000    0.000    0.000 reader.py:177(update_raw)\n        3    0.000    0.000    0.000    0.000 connection.py:533(Pipe)\n       43    0.000    0.000    0.000    0.000 scanner.py:1416(scan_line_break)\n        5    0.000    0.000    0.000    0.000 {built-in method posix.write}\n       22    0.000    0.000    0.000    0.000 resolver.py:143(resolve)\n        6    0.000    0.000    0.000    0.000 weakref.py:164(__setitem__)\n        1    0.000    0.000    0.000    0.000 tokenizer.py:69(init_vocab)\n        1    0.000    0.000    0.000    0.000 process.py:67(__init__)\n        4    0.000    0.000    0.000    0.000 process.py:234(ident)\n       24    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n        2    0.000    0.000    0.000    0.000 {method 'read' of '_io.TextIOWrapper' objects}\n       21    0.000    0.000    0.000    0.000 threading.py:271(__enter__)\n       26    0.000    0.000    0.000    0.000 scanner.py:125(peek_token)\n       11    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:405(parent)\n       20    0.000    0.000    0.000    0.000 scanner.py:295(save_possible_simple_key)\n        1    0.000    0.000    0.000    0.000 context.py:60(Pipe)\n        1    0.000    0.000    0.000    0.000 tokenizer.py:70(<dictcomp>)\n        1    0.000    0.000    0.000    0.000 weakref.py:427(__setitem__)\n        5    0.000    0.000    0.000    0.000 random.py:493(<listcomp>)\n        2    0.000    0.000    0.000    0.000 encoder.py:205(iterencode)\n        4    0.000    0.000    0.000    0.000 util.py:208(__call__)\n      241    0.000    0.000    0.000    0.000 scanner.py:264(next_possible_simple_key)\n       29    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n       17    0.000    0.000    0.000    0.000 iostream.py:90(_event_pipe)\n        1    0.000    0.000    0.000    0.000 threading.py:1051(_stop)\n        4    0.000    0.000    0.000    0.000 process.py:145(__init__)\n        4    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n       51    0.000    0.000    0.000    0.000 reader.py:94(prefix)\n       28    0.000    0.000    0.000    0.000 parser.py:114(get_event)\n        4    0.000    0.000    0.000    0.000 reader.py:146(update)\n        1    0.000    0.000    0.000    0.000 scanner.py:48(__init__)\n        1    0.000    0.000    0.000    0.000 {built-in method posix.cpu_count}\n       24    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n        4    0.000    0.000    0.000    0.000 _base.py:428(result)\n        8    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n        1    0.000    0.000    0.000    0.000 threading.py:1324(_make_invoke_excepthook)\n       21    0.000    0.000    0.000    0.000 threading.py:274(__exit__)\n        1    0.000    0.000    0.000    0.000 _base.py:155(_create_and_install_waiters)\n       78    0.000    0.000    0.000    0.000 error.py:6(__init__)\n        6    0.000    0.000    0.000    0.000 {built-in method posix.waitpid}\n        4    0.000    0.000    0.000    0.000 util.py:466(close_fds)\n        1    0.000    0.000    0.000    0.000 scanner.py:359(fetch_stream_start)\n       30    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n       10    0.000    0.000    0.000    0.000 <frozen abc>:117(__instancecheck__)\n        1    0.000    0.000    0.000    0.000 _base.py:77(__init__)\n        5    0.000    0.000    0.000    0.000 tempfile.py:282(rng)\n       15    0.000    0.000    0.000    0.000 constructor.py:402(construct_yaml_str)\n        1    0.000    0.000    0.000    0.000 parser.py:127(parse_stream_start)\n       20    0.000    0.000    0.000    0.000 constructor.py:173(construct_scalar)\n        6    0.000    0.000    0.000    0.000 weakref.py:347(__new__)\n        9    0.000    0.000    0.000    0.000 threading.py:283(_acquire_restore)\n       19    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n       13    0.000    0.000    0.000    0.000 threading.py:286(_is_owned)\n        8    0.000    0.000    0.000    0.000 process.py:94(<genexpr>)\n        8    0.000    0.000    0.000    0.000 process.py:99(_check_closed)\n       77    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n       33    0.000    0.000    0.000    0.000 scanner.py:325(unwind_indent)\n        5    0.000    0.000    0.000    0.000 connection.py:135(_check_closed)\n        3    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}\n        6    0.000    0.000    0.000    0.000 weakref.py:105(remove)\n        1    0.000    0.000    0.000    0.000 _base.py:61(__init__)\n        1    0.000    0.000    0.000    0.000 queues.py:71(_reset)\n        1    0.000    0.000    0.000    0.000 queue.py:34(__init__)\n       27    0.000    0.000    0.000    0.000 threading.py:575(is_set)\n        1    0.000    0.000    0.000    0.000 scanner.py:484(fetch_block_entry)\n        1    0.000    0.000    0.000    0.000 threading.py:811(_newname)\n        2    0.000    0.000    0.000    0.000 iostream.py:437(_is_master_process)\n        1    0.000    0.000    0.000    0.000 process.py:582(_check_system_limits)\n        1    0.000    0.000    0.000    0.000 contextlib.py:141(__exit__)\n        1    0.000    0.000    0.000    0.000 queues.py:348(close)\n        2    0.000    0.000    0.000    0.000 encoder.py:260(_make_iterencode)\n        1    0.000    0.000    0.000    0.000 threading.py:829(_maintain_shutdown_locks)\n        8    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.RLock' objects}\n        1    0.000    0.000    0.000    0.000 threading.py:429(__init__)\n        6    0.000    0.000    0.000    0.000 connection.py:118(__init__)\n        4    0.000    0.000    0.000    0.000 threading.py:90(RLock)\n        1    0.000    0.000    0.000    0.000 reader.py:138(check_printable)\n        4    0.000    0.000    0.000    0.000 threading.py:366(notify)\n        2    0.000    0.000    0.000    0.000 connection.py:174(close)\n        4    0.000    0.000    0.000    0.000 process.py:189(name)\n        4    0.000    0.000    0.000    0.000 constructor.py:233(construct_yaml_bool)\n        9    0.000    0.000    0.000    0.000 threading.py:280(_release_save)\n        6    0.000    0.000    0.000    0.000 _weakrefset.py:39(_remove)\n       16    0.000    0.000    0.000    0.000 threading.py:1168(ident)\n        4    0.000    0.000    0.000    0.000 __init__.py:228(_acquireLock)\n       15    0.000    0.000    0.000    0.000 scanner.py:731(check_plain)\n        4    0.000    0.000    0.000    0.000 queue.py:213(_put)\n       20    0.000    0.000    0.000    0.000 nodes.py:27(__init__)\n        2    0.000    0.000    0.000    0.000 connection.py:376(_close)\n        5    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n        1    0.000    0.000    0.000    0.000 contextlib.py:104(__init__)\n        1    0.000    0.000    0.000    0.000 contextlib.py:132(__enter__)\n        1    0.000    0.000    0.000    0.000 constructor.py:237(construct_yaml_int)\n        6    0.000    0.000    0.000    0.000 weakref.py:352(__init__)\n        2    0.000    0.000    0.000    0.000 constructor.py:405(construct_yaml_seq)\n        1    0.000    0.000    0.000    0.000 {method 'tell' of '_io.BufferedReader' objects}\n        4    0.000    0.000    0.000    0.000 {method 'remove' of 'set' objects}\n        3    0.000    0.000    0.000    0.000 <frozen codecs>:319(decode)\n        1    0.000    0.000    0.000    0.000 scanner.py:371(fetch_stream_end)\n        1    0.000    0.000    0.000    0.000 parser.py:159(parse_document_start)\n        6    0.000    0.000    0.000    0.000 {built-in method _weakref._remove_dead_weakref}\n       20    0.000    0.000    0.000    0.000 tokens.py:98(__init__)\n        7    0.000    0.000    0.000    0.000 {method 'match' of 're.Pattern' objects}\n       26    0.000    0.000    0.000    0.000 tokens.py:3(__init__)\n       20    0.000    0.000    0.000    0.000 events.py:65(__init__)\n        6    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x93e380}\n        2    0.000    0.000    0.000    0.000 events.py:22(__init__)\n       11    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n        2    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n       20    0.000    0.000    0.000    0.000 constructor.py:117(construct_scalar)\n        2    0.000    0.000    0.000    0.000 encoder.py:105(__init__)\n        1    0.000    0.000    0.000    0.000 constructor.py:124(construct_sequence)\n        1    0.000    0.000    0.000    0.000 constructor.py:180(flatten_mapping)\n       13    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n        1    0.000    0.000    0.000    0.000 _base.py:147(__enter__)\n       10    0.000    0.000    0.000    0.000 scanner.py:721(check_value)\n        5    0.000    0.000    0.000    0.000 synchronize.py:90(_make_methods)\n        1    0.000    0.000    0.000    0.000 parser.py:190(parse_document_end)\n       10    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n       40    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n        1    0.000    0.000    0.000    0.000 _base.py:144(__init__)\n        1    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n        1    0.000    0.000    0.000    0.000 parser.py:81(__init__)\n       22    0.000    0.000    0.000    0.000 resolver.py:91(descend_resolver)\n        1    0.000    0.000    0.000    0.000 <frozen codecs>:309(__init__)\n        4    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n       40    0.000    0.000    0.000    0.000 {built-in method math.floor}\n       10    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n       11    0.000    0.000    0.000    0.000 scanner.py:38(__init__)\n        8    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n       11    0.000    0.000    0.000    0.000 scanner.py:349(add_indent)\n       22    0.000    0.000    0.000    0.000 parser.py:107(peek_event)\n        3    0.000    0.000    0.000    0.000 events.py:5(__init__)\n       13    0.000    0.000    0.000    0.000 scanner.py:312(remove_possible_simple_key)\n        1    0.000    0.000    0.000    0.000 constructor.py:129(<listcomp>)\n       22    0.000    0.000    0.000    0.000 resolver.py:114(ascend_resolver)\n        6    0.000    0.000    0.000    0.000 context.py:197(get_start_method)\n        1    0.000    0.000    0.000    0.000 threading.py:839(<listcomp>)\n        1    0.000    0.000    0.000    0.000 weakref.py:369(remove)\n        1    0.000    0.000    0.000    0.000 tokenizer.py:28(__init__)\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n        1    0.000    0.000    0.000    0.000 context.py:237(get_context)\n        7    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n        2    0.000    0.000    0.000    0.000 nodes.py:36(__init__)\n        5    0.000    0.000    0.000    0.000 _base.py:223(<genexpr>)\n       11    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n        3    0.000    0.000    0.000    0.000 {method 'find' of 'bytes' objects}\n        5    0.000    0.000    0.000    0.000 util.py:48(debug)\n       26    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n        5    0.000    0.000    0.000    0.000 connection.py:143(_check_writable)\n        1    0.000    0.000    0.000    0.000 constructor.py:24(__init__)\n        2    0.000    0.000    0.000    0.000 threading.py:1206(daemon)\n       10    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n        2    0.000    0.000    0.000    0.000 <frozen codecs>:186(__init__)\n       11    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n        1    0.000    0.000    0.000    0.000 tokenizer.py:111(<listcomp>)\n        1    0.000    0.000    0.000    0.000 _base.py:151(__exit__)\n        6    0.000    0.000    0.000    0.000 connection.py:131(__del__)\n        3    0.000    0.000    0.000    0.000 {built-in method _codecs.utf_8_decode}\n        4    0.000    0.000    0.000    0.000 _base.py:398(__get_result)\n        1    0.000    0.000    0.000    0.000 {built-in method posix.sysconf}\n        5    0.000    0.000    0.000    0.000 process.py:37(current_process)\n        1    0.000    0.000    0.000    0.000 {method 'difference_update' of 'set' objects}\n        1    0.000    0.000    0.000    0.000 resolver.py:21(__init__)\n        6    0.000    0.000    0.000    0.000 context.py:187(get_context)\n        1    0.000    0.000    0.000    0.000 events.py:37(__init__)\n        1    0.000    0.000    0.000    0.000 events.py:46(__init__)\n        4    0.000    0.000    0.000    0.000 util.py:44(sub_debug)\n        2    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n        2    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n        2    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n        1    0.000    0.000    0.000    0.000 queue.py:206(_init)\n        1    0.000    0.000    0.000    0.000 composer.py:13(__init__)\n        1    0.000    0.000    0.000    0.000 parser.py:89(dispose)\n        1    0.000    0.000    0.000    0.000 events.py:55(__init__)\n        3    0.000    0.000    0.000    0.000 <frozen codecs>:331(getstate)\n        2    0.000    0.000    0.000    0.000 4062424040.py:9(perf_monitor)\n        4    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n        1    0.000    0.000    0.000    0.000 scanner.py:706(check_block_entry)\n        3    0.000    0.000    0.000    0.000 {method 'locked' of '_thread.lock' objects}\n        1    0.000    0.000    0.000    0.000 tokens.py:33(__init__)\n        1    0.000    0.000    0.000    0.000 <frozen codecs>:260(__init__)\n        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n        1    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n        1    0.000    0.000    0.000    0.000 scanner.py:690(check_document_start)\n        1    0.000    0.000    0.000    0.000 _base.py:643(__enter__)"},"metadata":{}}],"execution_count":27}]}